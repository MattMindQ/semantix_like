{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-31 10:17:52.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mLoading FastText model from 'backend/data/cc.fr.300.vec'...\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m fasttext_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend/data/cc.fr.300.vec\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading FastText model from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfasttext_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfasttext_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully with vocabulary size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mindex_to_key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 2. Define a helper function to create a concept vector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:2069\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2065\u001b[0m         _word2vec_read_binary(\n\u001b[0;32m   2066\u001b[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[0;32m   2067\u001b[0m         )\n\u001b[0;32m   2068\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2069\u001b[0m         \u001b[43m_word2vec_read_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(kv):\n\u001b[0;32m   2071\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduplicate words detected, shrinking matrix size from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2073\u001b[0m         kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(kv),\n\u001b[0;32m   2074\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1971\u001b[0m, in \u001b[0;36m_word2vec_read_text\u001b[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_word2vec_read_text\u001b[39m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding):\n\u001b[0;32m   1970\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line_no \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(vocab_size):\n\u001b[1;32m-> 1971\u001b[0m         line \u001b[38;5;241m=\u001b[39m fin\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[0;32m   1972\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1973\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from loguru import logger\n",
    "\n",
    "# 1. Load your French FastText model in .vec format\n",
    "fasttext_path = 'backend/data/cc.fr.300.vec'\n",
    "logger.info(f\"Loading FastText model from '{fasttext_path}'...\")\n",
    "model = KeyedVectors.load_word2vec_format(fasttext_path, binary=False)\n",
    "\n",
    "logger.info(f\"Model loaded successfully with vocabulary size: {len(model.index_to_key)}\\n\")\n",
    "\n",
    "# 2. Define a helper function to create a concept vector\n",
    "def create_concept_vector(\n",
    "    positive_words: list, \n",
    "    negative_words: list, \n",
    "    embedding_model: KeyedVectors\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a concept vector by summing the vectors of all\n",
    "    positive words and subtracting the vectors of all negative words.\n",
    "    \n",
    "    Args:\n",
    "        positive_words: List of words to add\n",
    "        negative_words: List of words to subtract\n",
    "        embedding_model: A Gensim KeyedVectors model with FastText vectors\n",
    "    \n",
    "    Returns:\n",
    "        A NumPy array representing the new concept vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        concept_vec = np.zeros(embedding_model.vector_size, dtype=np.float32)\n",
    "        \n",
    "        # Process positive words\n",
    "        for word in positive_words:\n",
    "            w = word.lower()\n",
    "            if w in embedding_model:\n",
    "                concept_vec += embedding_model[w]\n",
    "                logger.debug(f\"Added vector for positive word: {word}\")\n",
    "            else:\n",
    "                logger.warning(f\"Word not found in vocabulary: {word}\")\n",
    "\n",
    "        # Process negative words\n",
    "        for word in negative_words:\n",
    "            w = word.lower()\n",
    "            if w in embedding_model:\n",
    "                concept_vec -= embedding_model[w]\n",
    "                logger.debug(f\"Subtracted vector for negative word: {word}\")\n",
    "            else:\n",
    "                logger.warning(f\"Word not found in vocabulary: {word}\")\n",
    "\n",
    "        return concept_vec\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error creating concept vector: {e}\")\n",
    "        return np.zeros(embedding_model.vector_size, dtype=np.float32)\n",
    "\n",
    "# 3. Class to manage concept vectors\n",
    "class ConceptVectorManager:\n",
    "    def __init__(self, model: KeyedVectors):\n",
    "        self.model = model\n",
    "        self.concept_vectors = {}\n",
    "        logger.info(\"ConceptVectorManager initialized\")\n",
    "\n",
    "    def create_concept(\n",
    "        self,\n",
    "        name: str,\n",
    "        positive_words: list,\n",
    "        negative_words: list = None\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Creates and stores a new concept vector.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the concept\n",
    "            positive_words: List of words to add\n",
    "            negative_words: List of words to subtract (optional)\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if concept was created successfully\n",
    "        \"\"\"\n",
    "        try:\n",
    "            negative_words = negative_words or []\n",
    "            concept_vec = create_concept_vector(\n",
    "                positive_words=positive_words,\n",
    "                negative_words=negative_words,\n",
    "                embedding_model=self.model\n",
    "            )\n",
    "            \n",
    "            self.concept_vectors[name] = concept_vec\n",
    "            logger.info(f\"Created concept vector: {name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error creating concept {name}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_similar_words(self, concept_name: str, topn: int = 10) -> list:\n",
    "        \"\"\"\n",
    "        Find words most similar to a stored concept vector.\n",
    "        \n",
    "        Args:\n",
    "            concept_name: Name of the stored concept\n",
    "            topn: Number of similar words to return\n",
    "        \n",
    "        Returns:\n",
    "            List of (word, similarity) tuples\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if concept_name not in self.concept_vectors:\n",
    "                logger.error(f\"Concept not found: {concept_name}\")\n",
    "                return []\n",
    "                \n",
    "            results = self.model.similar_by_vector(\n",
    "                self.concept_vectors[concept_name], \n",
    "                topn=topn\n",
    "            )\n",
    "            logger.info(f\"Found {len(results)} similar words for concept: {concept_name}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error finding similar words for concept {concept_name}: {e}\")\n",
    "            return []\n",
    "\n",
    "# 4. Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the manager\n",
    "    manager = ConceptVectorManager(model)\n",
    "    \n",
    "    # Example: \"roi - homme + femme = reine\"\n",
    "    manager.create_concept(\n",
    "        name=\"royalty_female\",\n",
    "        positive_words=[\"roi\", \"femme\"],\n",
    "        negative_words=[\"homme\"]\n",
    "    )\n",
    "    \n",
    "    # Find similar words\n",
    "    results = manager.get_similar_words(\"royalty_female\", topn=10)\n",
    "    \n",
    "    print(\"\\nTop 10 words closest to the 'royalty_female' concept:\\n\")\n",
    "    for word, sim in results:\n",
    "        print(f\"{word} (similarity: {sim:.4f})\")\n",
    "\n",
    "    # Example: Create a concept for \"Paris without tourism\"\n",
    "    manager.create_concept(\n",
    "        name=\"paris_local\",\n",
    "        positive_words=[\"paris\", \"quotidien\", \"habitant\"],\n",
    "        negative_words=[\"touriste\", \"tourisme\"]\n",
    "    )\n",
    "    \n",
    "    results = manager.get_similar_words(\"paris_local\", topn=10)\n",
    "    \n",
    "    print(\"\\nTop 10 words closest to the 'paris_local' concept:\\n\")\n",
    "    for word, sim in results:\n",
    "        print(f\"{word} (similarity: {sim:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model from backend/data/cc.fr.300.vec\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def reduce_fasttext_model(input_path: str, output_path: str, n_words: int = 50000):\n",
    "    \"\"\"\n",
    "    Create a reduced version of the FastText model keeping only the most frequent words.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to the original .vec file\n",
    "        output_path: Where to save the reduced model\n",
    "        n_words: Number of words to keep\n",
    "    \"\"\"\n",
    "    print(f\"Loading original model from {input_path}\")\n",
    "    model = KeyedVectors.load_word2vec_format(input_path)\n",
    "    \n",
    "    # Get the vocabulary size and vector dimension\n",
    "    vocab_size = len(model.index_to_key)\n",
    "    vector_size = model.vector_size\n",
    "    \n",
    "    print(f\"Original model: {vocab_size} words, {vector_size} dimensions\")\n",
    "    \n",
    "    # Keep only the first n_words (they're already sorted by frequency in FastText)\n",
    "    reduced_words = model.index_to_key[:n_words]\n",
    "    \n",
    "    # Write the reduced model in word2vec format\n",
    "    print(f\"Writing reduced model to {output_path}\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        # Header: number of words and vector dimension\n",
    "        f.write(f\"{n_words} {vector_size}\\n\")\n",
    "        \n",
    "        # Write each word and its vector\n",
    "        for word in tqdm(reduced_words, desc=\"Writing vectors\"):\n",
    "            vector = model[word]\n",
    "            vector_str = ' '.join(f\"{x:.6f}\" for x in vector)\n",
    "            f.write(f\"{word} {vector_str}\\n\")\n",
    "    \n",
    "    print(f\"Created reduced model with {n_words} words\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reduce_fasttext_model(\n",
    "        input_path=\"backend/data/cc.fr.300.vec\",\n",
    "        output_path=\"data/cc.fr.300.reduced.vec\",\n",
    "        n_words=50000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
