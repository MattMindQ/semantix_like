{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-31 11:21:29.430\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mLoading FastText model from 'backend/data/cc.fr.300.reduced.vec'...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-31 11:22:03.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mModel loaded successfully with vocabulary size: 50000\n",
      "\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mConceptVectorManager initialized\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.479\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_concept_vector\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mAdded vector for positive word: roi\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.480\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_concept_vector\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mAdded vector for positive word: femme\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.481\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_concept_vector\u001b[0m:\u001b[36m47\u001b[0m - \u001b[34m\u001b[1mSubtracted vector for negative word: homme\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_concept\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mCreated concept vector: royalty_female\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_similar_words\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mFound 10 similar words for concept: royalty_female\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.682\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_concept_vector\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mAdded vector for positive word: paris\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.685\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_concept_vector\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mAdded vector for positive word: quotidien\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.687\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_concept_vector\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mAdded vector for positive word: habitant\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.690\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_concept_vector\u001b[0m:\u001b[36m47\u001b[0m - \u001b[34m\u001b[1mSubtracted vector for negative word: touriste\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.700\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_concept_vector\u001b[0m:\u001b[36m47\u001b[0m - \u001b[34m\u001b[1mSubtracted vector for negative word: tourisme\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_concept\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mCreated concept vector: paris_local\u001b[0m\n",
      "\u001b[32m2025-01-31 11:22:03.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_similar_words\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mFound 10 similar words for concept: paris_local\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words closest to the 'royalty_female' concept:\n",
      "\n",
      "roi (similarity: 0.9444)\n",
      "Roi (similarity: 0.7885)\n",
      "reine (similarity: 0.7228)\n",
      "monarque (similarity: 0.6957)\n",
      "prince (similarity: 0.6507)\n",
      "royaume (similarity: 0.6344)\n",
      "souverain (similarity: 0.6296)\n",
      "rois (similarity: 0.6225)\n",
      "princesse (similarity: 0.5896)\n",
      "duc (similarity: 0.5885)\n",
      "\n",
      "Top 10 words closest to the 'paris_local' concept:\n",
      "\n",
      "paris (similarity: 0.6702)\n",
      "Paris (similarity: 0.4485)\n",
      "PARIS (similarity: 0.4406)\n",
      "lyon (similarity: 0.4270)\n",
      "Villejuif (similarity: 0.4055)\n",
      "toulouse (similarity: 0.4046)\n",
      "marseille (similarity: 0.3937)\n",
      "Arcueil (similarity: 0.3911)\n",
      "Aubervilliers (similarity: 0.3827)\n",
      "Vanves (similarity: 0.3791)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from loguru import logger\n",
    "\n",
    "# 1. Load your French FastText model in .vec format\n",
    "fasttext_path = 'backend/data/cc.fr.300.reduced.vec'\n",
    "logger.info(f\"Loading FastText model from '{fasttext_path}'...\")\n",
    "model = KeyedVectors.load_word2vec_format(fasttext_path, binary=False)\n",
    "\n",
    "logger.info(f\"Model loaded successfully with vocabulary size: {len(model.index_to_key)}\\n\")\n",
    "\n",
    "# 2. Define a helper function to create a concept vector\n",
    "def create_concept_vector(\n",
    "    positive_words: list, \n",
    "    negative_words: list, \n",
    "    embedding_model: KeyedVectors\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a concept vector by summing the vectors of all\n",
    "    positive words and subtracting the vectors of all negative words.\n",
    "    \n",
    "    Args:\n",
    "        positive_words: List of words to add\n",
    "        negative_words: List of words to subtract\n",
    "        embedding_model: A Gensim KeyedVectors model with FastText vectors\n",
    "    \n",
    "    Returns:\n",
    "        A NumPy array representing the new concept vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        concept_vec = np.zeros(embedding_model.vector_size, dtype=np.float32)\n",
    "        \n",
    "        # Process positive words\n",
    "        for word in positive_words:\n",
    "            w = word.lower()\n",
    "            if w in embedding_model:\n",
    "                concept_vec += embedding_model[w]\n",
    "                logger.debug(f\"Added vector for positive word: {word}\")\n",
    "            else:\n",
    "                logger.warning(f\"Word not found in vocabulary: {word}\")\n",
    "\n",
    "        # Process negative words\n",
    "        for word in negative_words:\n",
    "            w = word.lower()\n",
    "            if w in embedding_model:\n",
    "                concept_vec -= embedding_model[w]\n",
    "                logger.debug(f\"Subtracted vector for negative word: {word}\")\n",
    "            else:\n",
    "                logger.warning(f\"Word not found in vocabulary: {word}\")\n",
    "\n",
    "        return concept_vec\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error creating concept vector: {e}\")\n",
    "        return np.zeros(embedding_model.vector_size, dtype=np.float32)\n",
    "\n",
    "# 3. Class to manage concept vectors\n",
    "class ConceptVectorManager:\n",
    "    def __init__(self, model: KeyedVectors):\n",
    "        self.model = model\n",
    "        self.concept_vectors = {}\n",
    "        logger.info(\"ConceptVectorManager initialized\")\n",
    "\n",
    "    def create_concept(\n",
    "        self,\n",
    "        name: str,\n",
    "        positive_words: list,\n",
    "        negative_words: list = None\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Creates and stores a new concept vector.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the concept\n",
    "            positive_words: List of words to add\n",
    "            negative_words: List of words to subtract (optional)\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if concept was created successfully\n",
    "        \"\"\"\n",
    "        try:\n",
    "            negative_words = negative_words or []\n",
    "            concept_vec = create_concept_vector(\n",
    "                positive_words=positive_words,\n",
    "                negative_words=negative_words,\n",
    "                embedding_model=self.model\n",
    "            )\n",
    "            \n",
    "            self.concept_vectors[name] = concept_vec\n",
    "            logger.info(f\"Created concept vector: {name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error creating concept {name}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_similar_words(self, concept_name: str, topn: int = 10) -> list:\n",
    "        \"\"\"\n",
    "        Find words most similar to a stored concept vector.\n",
    "        \n",
    "        Args:\n",
    "            concept_name: Name of the stored concept\n",
    "            topn: Number of similar words to return\n",
    "        \n",
    "        Returns:\n",
    "            List of (word, similarity) tuples\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if concept_name not in self.concept_vectors:\n",
    "                logger.error(f\"Concept not found: {concept_name}\")\n",
    "                return []\n",
    "                \n",
    "            results = self.model.similar_by_vector(\n",
    "                self.concept_vectors[concept_name], \n",
    "                topn=topn\n",
    "            )\n",
    "            logger.info(f\"Found {len(results)} similar words for concept: {concept_name}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error finding similar words for concept {concept_name}: {e}\")\n",
    "            return []\n",
    "\n",
    "# 4. Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the manager\n",
    "    manager = ConceptVectorManager(model)\n",
    "    \n",
    "    # Example: \"roi - homme + femme = reine\"\n",
    "    manager.create_concept(\n",
    "        name=\"royalty_female\",\n",
    "        positive_words=[\"roi\", \"femme\"],\n",
    "        negative_words=[\"homme\"]\n",
    "    )\n",
    "    \n",
    "    # Find similar words\n",
    "    results = manager.get_similar_words(\"royalty_female\", topn=10)\n",
    "    \n",
    "    print(\"\\nTop 10 words closest to the 'royalty_female' concept:\\n\")\n",
    "    for word, sim in results:\n",
    "        print(f\"{word} (similarity: {sim:.4f})\")\n",
    "\n",
    "    # Example: Create a concept for \"Paris without tourism\"\n",
    "    manager.create_concept(\n",
    "        name=\"paris_local\",\n",
    "        positive_words=[\"paris\", \"quotidien\", \"habitant\"],\n",
    "        negative_words=[\"touriste\", \"tourisme\"]\n",
    "    )\n",
    "    \n",
    "    results = manager.get_similar_words(\"paris_local\", topn=10)\n",
    "    \n",
    "    print(\"\\nTop 10 words closest to the 'paris_local' concept:\\n\")\n",
    "    for word, sim in results:\n",
    "        print(f\"{word} (similarity: {sim:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model from backend/data/cc.fr.300.vec\n",
      "Original model: 2000000 words, 300 dimensions\n",
      "Writing reduced model to backend/data/cc.fr.300.reduced.vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing vectors: 100%|██████████| 50000/50000 [00:37<00:00, 1321.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created reduced model with 50000 words\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def reduce_fasttext_model(input_path: str, output_path: str, n_words: int = 50000):\n",
    "    \"\"\"\n",
    "    Create a reduced version of the FastText model keeping only the most frequent words.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to the original .vec file\n",
    "        output_path: Where to save the reduced model\n",
    "        n_words: Number of words to keep\n",
    "    \"\"\"\n",
    "    print(f\"Loading original model from {input_path}\")\n",
    "    model = KeyedVectors.load_word2vec_format(input_path)\n",
    "    \n",
    "    # Get the vocabulary size and vector dimension\n",
    "    vocab_size = len(model.index_to_key)\n",
    "    vector_size = model.vector_size\n",
    "    \n",
    "    print(f\"Original model: {vocab_size} words, {vector_size} dimensions\")\n",
    "    \n",
    "    # Keep only the first n_words (they're already sorted by frequency in FastText)\n",
    "    reduced_words = model.index_to_key[:n_words]\n",
    "    \n",
    "    # Write the reduced model in word2vec format\n",
    "    print(f\"Writing reduced model to {output_path}\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        # Header: number of words and vector dimension\n",
    "        f.write(f\"{n_words} {vector_size}\\n\")\n",
    "        \n",
    "        # Write each word and its vector\n",
    "        for word in tqdm(reduced_words, desc=\"Writing vectors\"):\n",
    "            vector = model[word]\n",
    "            vector_str = ' '.join(f\"{x:.6f}\" for x in vector)\n",
    "            f.write(f\"{word} {vector_str}\\n\")\n",
    "    \n",
    "    print(f\"Created reduced model with {n_words} words\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reduce_fasttext_model(\n",
    "        input_path=\"backend/data/cc.fr.300.vec\",\n",
    "        output_path=\"backend/data/cc.fr.300.reduced.vec\",\n",
    "        n_words=50000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project Structure:\n",
      "================\n",
      "├── semantix_like/\n",
      "│   ├── LICENSE\n",
      "│   ├── README.md\n",
      "│   ├── description.md\n",
      "│   ├── docker-compose.yml\n",
      "│   ├── prompt.md\n",
      "│   ├── requirements.txt\n",
      "│   ├── todo\n",
      "│   ├── vercel.json\n",
      "│   ├── work.ipynb\n",
      "│   ├── backend/\n",
      "│   │   ├── 0.99)\n",
      "│   │   ├── Dict\n",
      "│   │   ├── Dockerfile\n",
      "│   │   ├── List[Dict]\n",
      "│   │   ├── None\n",
      "│   │   ├── app.log\n",
      "│   │   ├── app.py\n",
      "│   │   ├── requirements.txt\n",
      "│   │   ├── routes.py\n",
      "│   │   ├── str\n",
      "│   │   ├── test_config.py\n",
      "│   │   ├── config/\n",
      "│   │   │   ├── game_config.py\n",
      "│   │   ├── data/\n",
      "│   │   │   ├── cc.fr.300.reduced.vec\n",
      "│   │   │   ├── game_state.json\n",
      "│   │   │   ├── word_list.json\n",
      "│   │   ├── services/\n",
      "│   │   │   ├── game_service.py\n",
      "│   │   │   ├── model_downloader.py\n",
      "│   │   │   ├── visualization_service.py\n",
      "│   │   │   ├── word_service.py\n",
      "│   ├── frontend/\n",
      "│   │   ├── Dockerfile\n",
      "│   │   ├── index.html\n",
      "│   │   ├── package-lock.json\n",
      "│   │   ├── package.json\n",
      "│   │   ├── tailwind.config.js\n",
      "│   │   ├── src/\n",
      "│   │   │   ├── main.ts\n",
      "│   │   │   ├── types.ts\n",
      "│   │   │   ├── vite.config.ts\n",
      "│   │   │   ├── services/\n",
      "│   │   │   │   ├── api.ts\n",
      "│   │   │   ├── utils/\n",
      "│   │   │   │   ├── ui-updates.ts\n",
      "│   │   │   │   ├── visualization.ts\n",
      "│   │   │   │   ├── word-list-updates.ts\n",
      "│   ├── semantix-api/\n",
      "│   │   ├── Dockerfile\n",
      "│   │   ├── README.md\n",
      "│   │   ├── app.py\n",
      "│   │   ├── requirements.txt\n",
      "│   │   ├── config/\n",
      "│   │   │   ├── game_config.py\n",
      "│   │   ├── data/\n",
      "│   │   │   ├── game_state.json\n",
      "│   │   │   ├── word_list.json\n",
      "│   │   ├── services/\n",
      "│   │   │   ├── game_service.py\n",
      "│   │   │   ├── model_downloader.py\n",
      "│   │   │   ├── visualization_service.py\n",
      "│   │   │   ├── word_service.py\n",
      "\n",
      "Note: Excluded directories: .git, __pycache__, node_modules, env, venv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import List, Set\n",
    "\n",
    "def print_directory_structure(startpath: str, exclude_dirs: Set[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Print the directory structure starting from the specified path.\n",
    "    \n",
    "    Args:\n",
    "        startpath: The root directory to start from\n",
    "        exclude_dirs: Set of directory names to exclude\n",
    "    \"\"\"\n",
    "    if exclude_dirs is None:\n",
    "        exclude_dirs = {'.git', '__pycache__', 'node_modules', 'env', 'venv'}\n",
    "    \n",
    "    prefix = '│   '\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        # Skip excluded directories\n",
    "        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n",
    "        \n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = '│   ' * level\n",
    "        \n",
    "        folder_name = os.path.basename(root)\n",
    "        print(f'{indent}├── {folder_name}/')\n",
    "        \n",
    "        sub_indent = '│   ' * (level + 1)\n",
    "        for file in sorted(files):\n",
    "            if not file.startswith('.'):  # Skip hidden files\n",
    "                print(f'{sub_indent}├── {file}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the current directory or use command line argument\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    print(\"\\nProject Structure:\")\n",
    "    print(\"================\")\n",
    "    print_directory_structure(current_dir)\n",
    "    print(\"\\nNote: Excluded directories: .git, __pycache__, node_modules, env, venv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing /api/test/ping...\n",
      "Status Code: 200\n",
      "{'message': 'pong', 'status': 'ok'}\n",
      "\n",
      "Testing /api/test/model...\n",
      "Status Code: 200\n",
      "{'message': 'Model is working',\n",
      " 'status': 'ok',\n",
      " 'test_similarity': {'similarity': 0.5600714683532715,\n",
      "                     'word1': 'bonjour',\n",
      "                     'word2': 'salut'}}\n",
      "\n",
      "Testing /api/test/env...\n",
      "Status Code: 200\n",
      "{'environment': {'host': 'r-miroir-semantix-api-lwiuyzic-9b956-nvd2y',\n",
      "                 'model_url': 'https://huggingface.co/Miroir/cc.fr.300.reduced/resolve/main/cc.fr.300.reduced.vec',\n",
      "                 'python_version': '3.11.11',\n",
      "                 'services_initialized': {'game_service': True,\n",
      "                                          'visualization_service': True,\n",
      "                                          'word_service': True}},\n",
      " 'status': 'ok'}\n",
      "\n",
      "Testing /api/test/model-info...\n",
      "Status Code: 200\n",
      "{'model_info': {'sample_words': [',', 'de', '.', '</s>', 'la'],\n",
      "                'vocabulary_size': 50000},\n",
      " 'status': 'ok'}\n",
      "\n",
      "Summary:\n",
      "/api/test/ping: OK\n",
      "/api/test/model: OK\n",
      "/api/test/env: OK\n",
      "/api/test/model-info: OK\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Base URL for your Hugging Face Space\n",
    "BASE_URL = \"https://miroir-semantix-api.hf.space\"\n",
    "\n",
    "def test_endpoints():\n",
    "    \"\"\"Test all basic endpoints and print results\"\"\"\n",
    "    endpoints = [\n",
    "        \"/api/test/ping\",\n",
    "        \"/api/test/model\",\n",
    "        \"/api/test/env\",\n",
    "        \"/api/test/model-info\"\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for endpoint in endpoints:\n",
    "        print(f\"\\nTesting {endpoint}...\")\n",
    "        try:\n",
    "            response = requests.get(f\"{BASE_URL}{endpoint}\")\n",
    "            print(f\"Status Code: {response.status_code}\")\n",
    "            if response.status_code == 200:\n",
    "                pprint(response.json())\n",
    "                results[endpoint] = \"OK\"\n",
    "            else:\n",
    "                print(f\"Error: {response.text}\")\n",
    "                results[endpoint] = \"FAILED\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            results[endpoint] = f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    print(\"\\nSummary:\")\n",
    "    for endpoint, status in results.items():\n",
    "        print(f\"{endpoint}: {status}\")\n",
    "\n",
    "# Run the tests\n",
    "test_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
